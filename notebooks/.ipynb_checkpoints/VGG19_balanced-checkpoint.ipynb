{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "# Importing necessary packages\n",
    "import tempfile\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "from scripts.balanced_generator import BalancedDatasetGenerator\n",
    "from scripts.processed_generator import ProcessedDatasetGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising model arguments and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the parser and initialise arguments\n",
    "parser = argparse.ArgumentParser(description='COVID-CNN')\n",
    "\n",
    "parser.add_argument('--input_size', default=480, type=int, help='Dimensions of the input image')\n",
    "parser.add_argument('--n_channels', default=3, type=int, help='Number of channels in the image')\n",
    "parser.add_argument('--train_file', default='train_split.txt', type=str, help='Name of train metadata file')\n",
    "parser.add_argument('--test_file', default='test_split.txt', type=str, help='Name of test metadata file')\n",
    "parser.add_argument('--data_dir', default='data', type=str, help='Path to data folder containing datasets')\n",
    "parser.add_argument('--train_data_dir', default='train', type=str, help='Path to folder containing training dataset')\n",
    "parser.add_argument('--test_data_dir', default='test', type=str, help='Path to folder containing testing dataset')\n",
    "parser.add_argument('--num_classes', default=3, type=int, help='Number of classes in the dataset')\n",
    "parser.add_argument('--num_channels', default=3, type=int, help='Number of channels per image')\n",
    "parser.add_argument('--epochs', default=15, type=int, help='Number of epochs to train for')\n",
    "parser.add_argument('--bs', default=8, type=int, help='Batch size')\n",
    "parser.add_argument('--lr', default=0.0002, type=float, help='Learning rate')\n",
    "parser.add_argument(\"-p\", \"--plot\", type=str, default=\"plots/plot.png\", help=\"Path to save loss/accuracy plot\")\n",
    "parser.add_argument(\"-mp\", \"--model_plot\", type=str, default=\"plots/model_plot.png\", help=\"Path to save model's plot\")\n",
    "parser.add_argument(\"--model_dir\", type=str, default=\"saved_models/\", help=\"Path to save model\")\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"VGG19.h5\", help=\"Name of the model\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "# Declaring constants\n",
    "EPOCHS = args.epochs\n",
    "BS = args.bs\n",
    "LR = args.lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading training dataset csv and extracting labels\n",
    "df_train = pd.read_csv(args.train_file, sep=' ', header=0, index_col=None)\n",
    "train_labels = df_train['class']\n",
    "\n",
    "# Reading testing dataset csv and extracting labels\n",
    "df_test = pd.read_csv(args.test_file, sep=' ', header=0, index_col=None)\n",
    "test_labels = df_test['class']\n",
    "\n",
    "# Extracting unique classes\n",
    "classes = test_labels.unique()\n",
    "\n",
    "# Plotting distribution of classes in training dataset\n",
    "unique, counts = np.unique(train_labels, return_counts=True)\n",
    "plt.bar(unique, counts)\n",
    "\n",
    "plt.title('Class Frequency Training Dataset')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plotting distribution of classes in testing dataset\n",
    "unique, counts = np.unique(test_labels, return_counts=True)\n",
    "plt.bar(unique, counts)\n",
    "\n",
    "plt.title('Class Frequency Testing Dataset')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to add regularisation\n",
    "def add_regularization(model, regularizer=regularizers.l2(0.0001)):\n",
    "\n",
    "    if not isinstance(regularizer, regularizers.Regularizer):\n",
    "      print(\"Regularizer must be a subclass of tf.keras.regularizers.Regularizer\")\n",
    "      return model\n",
    "\n",
    "    for layer in model.layers:\n",
    "        for attr in ['kernel_regularizer']:\n",
    "            if hasattr(layer, attr):\n",
    "              setattr(layer, attr, regularizer)\n",
    "\n",
    "    # When we change the layers attributes, the change only happens in the model config file\n",
    "    model_json = model.to_json()\n",
    "\n",
    "    # Save the weights before reloading the model.\n",
    "    tmp_weights_path = os.path.join(tempfile.gettempdir(), 'tmp_weights.h5')\n",
    "    model.save_weights(tmp_weights_path)\n",
    "\n",
    "    # Load the model from the config\n",
    "    model = model_from_json(model_json)\n",
    "\n",
    "    # Reload the model weights\n",
    "    model.load_weights(tmp_weights_path, by_name=True)\n",
    "    return model\n",
    "\n",
    "def create_model():\n",
    "    # Loading VGG19 model trained on imagenet without head\n",
    "    baseModel = VGG19(\n",
    "        weights=\"imagenet\", include_top=False,\n",
    "        input_tensor=Input(shape=(args.input_size, args.input_size,\n",
    "                                  args.n_channels))\n",
    "    )\n",
    "\n",
    "    # Constructing the head for classification (to be trained)\n",
    "    headModel = baseModel.output\n",
    "    headModel = AveragePooling2D(pool_size=(4, 4))(headModel)\n",
    "    headModel = Flatten(name=\"flatten\")(headModel)\n",
    "    headModel = Dense(64, activation=\"relu\")(headModel)\n",
    "    headModel = Dropout(0.5)(headModel)\n",
    "    headModel = Dense(3, activation=\"softmax\")(headModel)\n",
    "\n",
    "    # Fixing the weights of base model\n",
    "    baseModel.trainable = False\n",
    "    \n",
    "    #  Adding regularisation\n",
    "    model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "    model = add_regularization(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating data generators to load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = ProcessedDatasetGenerator(\n",
    "        data_dir=os.path.join(args.data_dir),\n",
    "        input_shape=(args.input_size, args.input_size),\n",
    "        num_channels=args.num_channels,\n",
    "        is_validation = False,\n",
    "        data_files=df_test.to_csv(header=None, index=False, sep=\" \").strip('\\n').split(\"\\n\")\n",
    ")\n",
    "\n",
    "\n",
    "def get_model(df_trn, df_val):\n",
    "    model = create_model()\n",
    "    \n",
    "    trn_generator = BalancedDatasetGenerator(\n",
    "        data_dir=os.path.join(args.data_dir),\n",
    "        data_files=df_trn.to_csv(header=None, index=False, sep=\" \").strip('\\n').split(\"\\n\"),\n",
    "        batch_size=BS,\n",
    "        input_shape=(args.input_size, args.input_size),\n",
    "        num_channels=args.num_channels,\n",
    "        class_weights = [1.,1.,6.]\n",
    "    )\n",
    "\n",
    "    val_generator = ProcessedDatasetGenerator(\n",
    "        data_dir=os.path.join(args.data_dir),\n",
    "        input_shape=(args.input_size, args.input_size),\n",
    "        num_channels=args.num_channels,\n",
    "        is_validation=True,\n",
    "        data_files=df_val.to_csv(header=None, index=False, sep=\" \").strip('\\n').split(\"\\n\")\n",
    "    )\n",
    "\n",
    "    return model, trn_generator, val_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, _, _ = train_test_split(df_train, df_train['class'], test_size=0.33, random_state=42)\n",
    "\n",
    "# Getting model and generators fitted on ith fold data\n",
    "model, train_generator, validation_generator = get_model(X_train, X_test)\n",
    "\n",
    "# Compiling the model\n",
    "opt = Adam(learning_rate=LR)\n",
    "model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=['accuracy','AUC'])\n",
    "\n",
    "train_classes = pd.DataFrame(train_generator.classes)\n",
    "n_train = len(train_classes[train_classes.isin(['normal', 'pneumonia'])])\n",
    "\n",
    "val_classes = pd.DataFrame(validation_generator.classes)\n",
    "n_val = len(val_classes[val_classes.isin(['normal', 'pneumonia'])])\n",
    "\n",
    "# CREATE CALLBACKS\n",
    "checkpoint = ModelCheckpoint(args.model_dir+args.model_name, \n",
    "                        monitor='val_auc', verbose=1, \n",
    "                        save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_auc', mode='max', verbose=1, patience=2)\n",
    "callbacks_list = [checkpoint, es]\n",
    "\n",
    "# Training model\n",
    "print(\"[INFO] Training model\")\n",
    "H = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator),\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list,\n",
    "    class_weight={0:1,1:1,2:6},\n",
    "    use_multiprocessing=True, workers=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting training loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training loss and accuracy\n",
    "N = len(H.history[\"loss\"])\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on COVIDx\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(args.plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model and generating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on testing data\n",
    "print(\"[INFO] Predicting on testing data\")\n",
    "\n",
    "nb_samples = len(df_test[df_test['class'].isin(['normal','pneumonia'])])\n",
    "# Loading best model\n",
    "pred = model.predict(test_generator, steps=len(test_generator), verbose=1)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "\n",
    "# Getting class label map\n",
    "temp = []\n",
    "for i in test_labels:\n",
    "    temp.append(train_generator.mapping.get(i))\n",
    "\n",
    "# Printing classification report\n",
    "print(classification_report(temp, pred, target_names=train_generator.mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to plot confusion matrix\n",
    "Citation: https://www.kaggle.com/grfiv4/plot-a-confusion-matrix\n",
    "'''\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating confusion matrix and calculating: accuracy, sensitivity and specificity\n",
    "cm = confusion_matrix(temp, pred)\n",
    "total = sum(sum(cm))\n",
    "acc = (cm[0, 0] + cm[1, 1] + cm[2, 2]) / total\n",
    "sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1] + cm[0, 2])\n",
    "TN = cm[1, 1] + cm[1, 2] + cm[2, 1] + cm[2, 2]\n",
    "FP = cm[1, 0] + cm[2, 0]\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "# Printing found values\n",
    "print(cm)\n",
    "print(\"Overall Accuracy: {:.4f}\".format(acc))\n",
    "print(\"COVID Sensitivity: {:.4f}\".format(sensitivity))\n",
    "print(\"COVID Specificity: {:.4f}\".format(specificity))\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(cm, \n",
    "                      normalize    = False,\n",
    "                      target_names = list(train_generator.mapping.keys()),\n",
    "                      title        = \"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
