{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and running the ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary ilbraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scripts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c2d740ce3528>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble_balanced_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBalancedDatasetGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble_processed_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProcessedDatasetGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scripts'"
     ]
    }
   ],
   "source": [
    "# Configuring for execution support\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "# Suppressing warnings\n",
    "import logging\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "\n",
    "# Importing necessary packages\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "from scripts.ensemble_balanced_generator import BalancedDatasetGenerator\n",
    "from scripts.ensemble_processed_generator import ProcessedDatasetGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising model arguments and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the parser and initialise arguments\n",
    "parser = argparse.ArgumentParser(description='COVID-CNN')\n",
    "\n",
    "parser.add_argument('--input_size', default=480, type=int, help='Dimensions of the input image')\n",
    "parser.add_argument('--n_channels', default=3, type=int, help='Number of channels in the image')\n",
    "parser.add_argument(\"--model_dir\", type=str, default=\"saved_models/\", help=\"Path to save model\")\n",
    "parser.add_argument('--n_models', default=10, type=int, help='Number of models in the ensemble')\n",
    "parser.add_argument('--model_name', default='ensemble_model', type=str, help='Name of the ensemble model')\n",
    "parser.add_argument('--train_file', default='train_split.txt', type=str, help='Name of train metadata file')\n",
    "parser.add_argument('--test_file', default='test_split.txt', type=str, help='Name of test metadata file')\n",
    "parser.add_argument('--data_dir', default='data', type=str, help='Path to data folder containing datasets')\n",
    "parser.add_argument('--train_data_dir', default='train', type=str, help='Path to folder containing training dataset')\n",
    "parser.add_argument('--test_data_dir', default='test', type=str, help='Path to folder containing testing dataset')\n",
    "parser.add_argument('--num_classes', default=3, type=int, help='Number of classes in the dataset')\n",
    "parser.add_argument('--num_channels', default=3, type=int, help='Number of channels per image')\n",
    "parser.add_argument('--epochs', default=15, type=int, help='Number of epochs to train for')\n",
    "parser.add_argument('--bs', default=8, type=int, help='Batch size')\n",
    "parser.add_argument('--lr', default=0.0002, type=float, help='Learning rate')\n",
    "parser.add_argument(\"-p\", \"--plot\", type=str, default=\"plots/plot.png\", help=\"Path to save loss/accuracy plot\")\n",
    "parser.add_argument(\"-mp\", \"--model_plot\", type=str, default=\"plots/ensemble_model.png\", help=\"Path to save model's plot\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "# Declaring constants\n",
    "EPOCHS = args.epochs\n",
    "BS = args.bs\n",
    "LR = args.lr\n",
    "N_MODELS=args.n_models\n",
    "MODELS=['VGG16',\n",
    "        'DenseNet121',\n",
    "        'Xception',\n",
    "        'VGG19',\n",
    "        'MobileNet',\n",
    "        'InceptionV3',\n",
    "        'ResNet101V2',\n",
    "        'DenseNet201',\n",
    "        'ResNet50V2',\n",
    "        'ResNet152V2']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to load all method weights and creating the stacked model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading weights of all models\n",
    "def load_all_models(n_models):\n",
    "    all_models = list()\n",
    "    for i in range(n_models):\n",
    "        # Defining the location of model\n",
    "        filename = os.path.join(args.model_dir,MODELS[i]+'.h5')\n",
    "        # Loading model from file\n",
    "        model = load_model(filename)\n",
    "        # Making all model layers non-trainable\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "            # Renaming to avoid 'unique layer name' issue\n",
    "            layer._name = MODELS[i] + '_' + layer.name\n",
    "        # Adding to the list of all models\n",
    "        all_models.append(model)\n",
    "        print('[INFO] Loaded: {}'.format(MODELS[i]))\n",
    "    print('[INFO] Loaded all models')\n",
    "    return all_models\n",
    "\n",
    "def create_model(members):  \n",
    "    # Defining ensemble inputs\n",
    "    ensemble_inputs = [model.input for model in members]\n",
    "    # Merging outputs of each model\n",
    "    ensemble_outputs = [model.output for model in members]\n",
    "    merge = concatenate(ensemble_outputs)\n",
    "    # Adding layers to be trained\n",
    "    hidden = Dense(10, activation='relu')(merge)\n",
    "    output = Dense(3, activation='softmax')(hidden)\n",
    "    model = Model(inputs=ensemble_inputs, outputs=output)\n",
    "    model([ensemble_inputs,output])\n",
    "    # Plotting the ensemble model\n",
    "    #plot_model(model, show_shapes=True, to_file=args.model_plot)\n",
    "    # Compiling the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', 'AUC'])\n",
    "    #model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the COVIDx dataset files and creating generators to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading training dataset csv and extracting labels\n",
    "df_train = pd.read_csv(args.train_file, sep=' ', header=0, index_col=None)\n",
    "train_labels = df_train['class']\n",
    "\n",
    "# Reading testing dataset csv and extracting labels\n",
    "df_test = pd.read_csv(args.test_file, sep=' ', header=0, index_col=None)\n",
    "test_labels = df_test['class']\n",
    "\n",
    "# Extracting unique classes\n",
    "classes = test_labels.unique()\n",
    "\n",
    "test_generator = ProcessedDatasetGenerator(\n",
    "        data_dir=os.path.join(args.data_dir),\n",
    "        input_shape=(args.input_size, args.input_size),\n",
    "        num_channels=args.num_channels,\n",
    "        is_validation = False,\n",
    "        data_files=df_test.to_csv(header=None, index=False, sep=\" \").strip('\\n').split(\"\\n\")\n",
    ")\n",
    "\n",
    "\n",
    "def get_generators(df_trn, df_val):\n",
    "    trn_generator = BalancedDatasetGenerator(\n",
    "        data_dir=os.path.join(args.data_dir),\n",
    "        data_files=df_trn.to_csv(header=None, index=False, sep=\" \").strip('\\n').split(\"\\n\"),\n",
    "        batch_size=BS,\n",
    "        input_shape=(args.input_size, args.input_size),\n",
    "        num_channels=args.num_channels,\n",
    "        class_weights = [1.,1.,6.]\n",
    "    )\n",
    "\n",
    "    val_generator = ProcessedDatasetGenerator(\n",
    "        data_dir=os.path.join(args.data_dir),\n",
    "        input_shape=(args.input_size, args.input_size),\n",
    "        num_channels=args.num_channels,\n",
    "        is_validation=True,\n",
    "        data_files=df_val.to_csv(header=None, index=False, sep=\" \").strip('\\n').split(\"\\n\")\n",
    "    )\n",
    "\n",
    "    return trn_generator, val_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to fit and predict using stacked model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fit the model\n",
    "def fit_stacked_model(model):\n",
    "    X_train, X_test, _, _ = train_test_split(df_train, df_train['class'], test_size=0.33, random_state=42)\n",
    "    \n",
    "    # Getting generators\n",
    "    train_generator, validation_generator = get_generators(X_train, X_test)\n",
    "    \n",
    "    # CREATE CALLBACKS\n",
    "    checkpoint = ModelCheckpoint(args.model_dir+args.model_name, \n",
    "                            monitor='val_auc', verbose=1, \n",
    "                            save_best_only=True, mode='max')\n",
    "    es = EarlyStopping(monitor='val_auc', mode='max', verbose=1, patience=2)\n",
    "    callbacks_list = [checkpoint, es]\n",
    "\n",
    "    # Training model\n",
    "    print(\"[INFO] Training the stacked model\")\n",
    "    H = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks_list,\n",
    "        use_multiprocessing=True, workers=8)\n",
    "    \n",
    "    # Plotting training loss and accuracy\n",
    "    N = len(H.history[\"loss\"])\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "    plt.title(\"Training Loss and Accuracy on COVIDx\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.savefig(args.plot)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def predict_stacked_model(model, test_generator):\n",
    "    return model.predict(test_generator, steps=len(test_generator), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "members = load_all_models(args.n_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=[]\n",
    "for mem in members:\n",
    "    p=predict_stacked_model(mem, test_generator)\n",
    "    pred.append(p)\n",
    "    \n",
    "# sum across ensemble members\n",
    "summed = np.sum(pred, axis=0)\n",
    "# argmax across classes\n",
    "result = np.argmax(summed, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting class label map\n",
    "temp = []\n",
    "for i in test_labels:\n",
    "    temp.append(test_generator.mapping.get(i))\n",
    "\n",
    "print(classification_report(temp, result, target_names=test_generator.mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to plot confusion matrix\n",
    "Citation: https://www.kaggle.com/grfiv4/plot-a-confusion-matrix\n",
    "'''\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating confusion matrix and calculating: accuracy, sensitivity and specificity\n",
    "cm = confusion_matrix(temp, result)\n",
    "total = sum(sum(cm))\n",
    "acc = (cm[0, 0] + cm[1, 1] + cm[2, 2]) / total\n",
    "sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1] + cm[0, 2])\n",
    "TN = cm[1, 1] + cm[1, 2] + cm[2, 1] + cm[2, 2]\n",
    "FP = cm[1, 0] + cm[2, 0]\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "# Printing found values\n",
    "print(cm)\n",
    "print(\"Overall Accuracy: {:.4f}\".format(acc))\n",
    "print(\"COVID Sensitivity: {:.4f}\".format(sensitivity))\n",
    "print(\"COVID Specificity: {:.4f}\".format(specificity))\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(cm, \n",
    "                      normalize    = False,\n",
    "                      target_names = list(test_generator.mapping.keys()),\n",
    "                      title        = \"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
